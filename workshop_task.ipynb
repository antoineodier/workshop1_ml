{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "workshop_task.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antoineodier/workshop1_ml/blob/master/workshop_task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHbQZnnRCIbf",
        "colab_type": "text"
      },
      "source": [
        "# Exploratory Data Analysis and Feature Engineering Workshop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzRBLUpXCIbg",
        "colab_type": "text"
      },
      "source": [
        "In this workshop you will learn about different methods of data preprocessing and analysis.\n",
        "\n",
        "Agenda:\n",
        "\n",
        "1. Cleaning and transforming data, preparing it for further processing: removing missing values, correcting errors, removing outliers and errors, converting data types, etc. \n",
        "\n",
        "2. Exploratory data analysis: correlation analysis, variable relationship analysis. \n",
        "\n",
        "3. Preparing the feature space: encoding categorical variables, manual feature generation, dimensionality reduction and feature selection.\n",
        "\n",
        "4. Training the model.\n",
        "\n",
        "For this workshop we will be using a dataset, which contains records of approximately 10 000 applications scraped from Google Play Store. This  will allow us to demonstrate different data processing and feature engineering techniques in conditions similar to a real project (such, that data scientists have to deal with in their work all the time).\n",
        "We will try to answer the question: what factors determine the rating of a particular application and whether this rating can be predicted in advance?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0tZgKraCIbi",
        "colab_type": "text"
      },
      "source": [
        "<a id='attention'></a>\n",
        "\n",
        "**!! Attention !!**\n",
        "\n",
        "In order to slightly simplify the process, one assumption was made. We will perform all actions on the whole dataset and will split it into training and test subsets only before actually fitting the model. This approach allows us to avoid writing a large number of functions to convert the test dataset to the format of the training and simplifies the understanding of the performed actions. \n",
        "\n",
        "However, it is only acceptable on educational tasks. When working with real data, test and validation datasets should be allocated in advance. All data analysis is performed only on the training subset (except checking data distributions). The validation and test subsets are transformed to the training format (to the same format: only those categories that are in the training set are left, missing values are processed in the same way, etc.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWm7fAsoCIbj",
        "colab_type": "text"
      },
      "source": [
        "<a id='0'></a>\n",
        "### Table of contents\n",
        "\n",
        "[Libraries](#libraries)\n",
        "\n",
        "+ [data preprocessing](#lib_data_manipulation)\n",
        "+ [data visualization](#lib_data_visualization)\n",
        "+ [machine learning](#lib_machine_learning)\n",
        "+ [text mining](#lib_text)\n",
        "\n",
        "[1. Exploring and transforming the variables](#1_data_cleaning)\n",
        "\n",
        "+ [Brief data overview](#surface_examination)\n",
        "\n",
        "+ [Exploring the data](#data_studying)\n",
        "\n",
        "    + [Category](#category)\n",
        "    + [Rating](#rating)\n",
        "    + [Size](#size)\n",
        "    + [Type](#type)\n",
        "    + [Price](#price)\n",
        "    + [Content Rating](#content_rating)\n",
        "    + [Genres](#genres)\n",
        "    + [Current Version](#current_ver)\n",
        "    + [Android Version](#android_ver)\n",
        "    + [Final check](#final_check)\n",
        "    + [Removing duplicates](#removing_duplicates)\n",
        "\n",
        "[2. Exploring the relationships between variables](#2_data_relations)\n",
        "\n",
        "+ [Numerical features](#numerical_features)\n",
        "+ [Application length and rating](#app_len_&_rating)\n",
        "+ [Price and Type](#type_&_price)\n",
        "+ [Category and Genre](#category_&_genres)\n",
        "+ [Rating and Content Rating](#rating_&_content_rating)\n",
        "+ [Size and Content Rating](#size_&_rating)\n",
        "+ [Category and Price](#category_&_price)\n",
        "    \n",
        "[3. Working with feature space](#3_feature_space)\n",
        "\n",
        "+ [Base model](#base_model)\n",
        "+ [Basic features](#origin_features)\n",
        "+ [Feature generation](#feature_generation)\n",
        "  + [Outlier removal](#outlier_removal)  \n",
        "+ [Dimensionality reduction](#dimensional_reduction)\n",
        "  + [Principal Component Analysis](#pca)\n",
        "  + [Greedy selection](#greedy_selection)\n",
        "\n",
        "[4. Final prediction](#4_final_prediction)\n",
        "+ [Conclusion](#conclusions)\n",
        "+ [Bonus](#bonus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1R5V4LgCIbk",
        "colab_type": "text"
      },
      "source": [
        "We will use the libraries dividing into the following categories:\n",
        "+ [data preprocessing and descriptive statistics](#lib_data_manipulation)\n",
        "+ [data visualization](#lib_data_visualization)\n",
        "+ [machine learning](#lib_machine_learning)\n",
        "+ [text mining](#lib_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aVhYf3UCIbm",
        "colab_type": "text"
      },
      "source": [
        "<a id='lib_data_manipulation'></a>\n",
        "#### data preprocessing and descriptive statistics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcnCsS96CIbn",
        "colab_type": "text"
      },
      "source": [
        "> **pandas** - a library for data manipulation: dataset transformation, descriptive statistics, working with time series, simple visualization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XULXvJPiCIbo",
        "colab_type": "text"
      },
      "source": [
        "> **numpy** - a library for working with numeric arrays, convenient in combination with pandas, as it uses less resources."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVVU6GyqCIbp",
        "colab_type": "text"
      },
      "source": [
        "> **scipy** - a library that allows applying mathematical functions, linear algebra, statistical tests to data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKgKVmvaCIbq",
        "colab_type": "text"
      },
      "source": [
        ">**statsmodels** - an add-on over SciPy that simplifies the application of some statistical models and makes visualization more illustrative."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zcXrDtoCIbr",
        "colab_type": "text"
      },
      "source": [
        ">**featuretools** - a library for automatic features generation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aodxyjb_CIbs",
        "colab_type": "text"
      },
      "source": [
        "<a id='lib_data_visualization'></a>\n",
        "#### data visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_o8IbEECIbt",
        "colab_type": "text"
      },
      "source": [
        "> **matplotlib** - the main library for data visualization, has a rich functionality, although interface is a little bit outdated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4p7-nFxYCIbu",
        "colab_type": "text"
      },
      "source": [
        "> **seaborn** - an add-on over matplotlib, makes graphics more informative and visually attractive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rQyYtznCIbv",
        "colab_type": "text"
      },
      "source": [
        "> **wordcloud** - a library for visual representation of text as a tag cloud."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXIpjzKoCIbw",
        "colab_type": "text"
      },
      "source": [
        "<a id='lib_machine_learning'></a>\n",
        "#### machine learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1Mn2EUnCIbx",
        "colab_type": "text"
      },
      "source": [
        "> **scikit-learn (sklearn)** - a library containing a large number of algorithms for solving machine learning problems: data preprocessing, implementation of different models, toy datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vci8bBfnCIby",
        "colab_type": "text"
      },
      "source": [
        ">**umap** - implementation of the \"Uniform manifest Approximation and Projection\" algorithm. Allows you to visualize multidimensional data by lowering its dimension"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5_TEVSbCIby",
        "colab_type": "text"
      },
      "source": [
        "<a id='lib_text'></a>\n",
        "#### text mining"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpcjfraKCIbz",
        "colab_type": "text"
      },
      "source": [
        "> **nltk** - one of the popular platforms for working with text data, includes text processing algorithms, machine learning models for working with text and much more."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GXCLFy_CIb0",
        "colab_type": "text"
      },
      "source": [
        "> **re** - a library for working with regular expressions.\n",
        "\n",
        "> > *Regular expressions* - a formal language for searching and manipulating substrings in the text, based on the use of wildcard characters. For text manipulations, a replacement string is additionally specified, which can also contain special characters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itjSINnvCIb0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from numpy.random import seed\n",
        "\n",
        "import scipy.stats as stats\n",
        "from scipy.stats import uniform, truncnorm, randint\n",
        "import statsmodels.api as sm\n",
        "import random\n",
        "\n",
        "import pandas as pd\n",
        "import featuretools as ft\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.metrics import f1_score, classification_report, confusion_matrix, accuracy_score, roc_auc_score\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, make_scorer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import IsolationForest\n",
        "import sklearn.cluster as cluster\n",
        "import xgboost as xgb\n",
        "import umap\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from pylab import rcParams\n",
        "import seaborn as sns\n",
        "plt.style.use('seaborn-poster')\n",
        "%matplotlib inline\n",
        "\n",
        "import os \n",
        "import pickle\n",
        "import warnings\n",
        "from time import time\n",
        "warnings.filterwarnings('ignore') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5u6781WCIb3",
        "colab_type": "text"
      },
      "source": [
        "[Table of contents](#0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQICQtlkCIb3",
        "colab_type": "text"
      },
      "source": [
        "<a id='1_data_cleaning'></a>\n",
        "## 1. Exploring and  transforming the variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kf6ON-iCIb4",
        "colab_type": "text"
      },
      "source": [
        "<a id='surface_examination'></a>\n",
        "### Brief data overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9MnCHUpCIb4",
        "colab_type": "text"
      },
      "source": [
        "First, we should load the data and visually explore at least a few lines, see the types of variables, the number of samples in order to get the general understanding of data and get the feeling of what you are working with."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woP4CdyYCIb5",
        "colab_type": "text"
      },
      "source": [
        "Let's define the paths to the data. It is better to do this at the beginning so, if in the future we need to change them, you won't have to search through the entire script."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLgHmXxJCIb6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_F ='https://raw.githubusercontent.com/Shroedinger/workshop_fe/master/short_version_rus/googleplaystore_alter.csv'\n",
        "FEATURE_IMPORTANCE_F= 'https://raw.githubusercontent.com/Shroedinger/workshop_fe/master/short_version_rus/feature_importances_logreg.csv'\n",
        "BONUS_F = 'https://raw.githubusercontent.com/Shroedinger/workshop_fe/master/short_version_rus/bonus_df_alter.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJHPT4YMCIb9",
        "colab_type": "text"
      },
      "source": [
        "Construction \"../\" means - one level higher relative to the script. In our case we exit the folder, where the script file is located, and go to the \"data\" folder. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnpidPMSCIb9",
        "colab_type": "text"
      },
      "source": [
        "Pandas uses two data structures to store data:\n",
        "\n",
        "**Series** - is an object similar to ordinary one-dimensional array, with the difference that each element has its own index. Moreover, Series has a stored data type.\n",
        "\n",
        "**DataFrame** - is a tabular data structure where each column is a Series.\n",
        "\n",
        "Let's load *googleplaystore_alter.csv* into variable *df* with *pandas* function *read_csv()*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-KpUgrICIb-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(DATA_F)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITbIBZPGCIcA",
        "colab_type": "text"
      },
      "source": [
        "**Task 1:** Bring out the first (or last) **ten** rows from dataset using methods [head()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.head.html) or [tail()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.tail.html#pandas.DataFrame.tail). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLFQDE8eCIcA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# write your code here\n",
        "df.head(10)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iXj3sApCIcD",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "**Dataset Descriptions:**\n",
        "\n",
        "+ App - application name;\n",
        "\n",
        "+ Category - application category;\n",
        "\n",
        "+ Rating - application rating;\n",
        "\n",
        "+ Size - application size;\n",
        "\n",
        "+ Type - application type (paid or free);\n",
        "\n",
        "+ Price - application price;\n",
        "\n",
        "+ Content Rating - the age group for which the application is intended;\n",
        "\n",
        "+ Genres - subcategories to which the application belongs;\n",
        "\n",
        "+ Current Ver - current version of application;\n",
        "\n",
        "+ Android Ver - the minimum OS version required for the application works.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcoiT4vjCIcD",
        "colab_type": "text"
      },
      "source": [
        "*DataFrame* objects have dual indexing - by columns and by rows. \n",
        "To select a specific row you can use *loc* (returns rows by a given index) or *iloc* (returns rows by their position in the dataset) methods. It should be noticed that loc and iloc methods are most likely to return different values. For example, let's select the 3rd line:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMClD-ODCIcE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.iloc[2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I970ClL8CIcG",
        "colab_type": "text"
      },
      "source": [
        "We can also select a specific column:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvSPUFPOCIcG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['App'].head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ic-e5ZwWCIcI",
        "colab_type": "text"
      },
      "source": [
        "The *at* method is used for quick selection of the specific element:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sFI2ynwCIcJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.at[1, 'App']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQrPX3q-CIcL",
        "colab_type": "text"
      },
      "source": [
        "If you need to select several columns, you could do it by feeding a list. For several rows you can use a list or a slice:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJNmcrSSCIcL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.loc[1:3, ['App', 'Category']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGBWPWxPCIcN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.iloc[2:4, [1, 2]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVjqb-ofCIcQ",
        "colab_type": "text"
      },
      "source": [
        "If you need strings that match a condition (for example, only applications of the ART_AND_DESIGN category), the following query format is used (it also returns a DataFrame object):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opwx4PI1CIcR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df[df['Category']=='ART_AND_DESIGN'].head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wSpkbI3CIcT",
        "colab_type": "text"
      },
      "source": [
        "For complex conditions, each expression is placed in parentheses and the & or | operators are used. For example, we can find the applications from the category ART_AND_DESIGN with the type Free:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbS4v-2XCIcU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df[(df['Category']=='ART_AND_DESIGN') & (df.Type == 'Free')].head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-WnlLiwCIce",
        "colab_type": "text"
      },
      "source": [
        "Let's take a look at the general information about dataset using the *info()* method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNJ4YawCCIcj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZzVDCuGCIco",
        "colab_type": "text"
      },
      "source": [
        "The dataset consists of 13 columns, only one of which is numeric (float64) and the rest are categorical (object). Dataset has 10841 rows. There are missing values, mostly in the Current Ver."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RRTy-UACIcp",
        "colab_type": "text"
      },
      "source": [
        "*Pandas* contains many built-in functions for working with data. You can learn more about them here: [pandas](https://pandas.pydata.org/pandas-docs/stable/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpfOoYTsCIcr",
        "colab_type": "text"
      },
      "source": [
        "[Table of contents](#0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVffYc-MCIcs",
        "colab_type": "text"
      },
      "source": [
        "<a id='data_studying'></a>\n",
        "### Exploring the data \n",
        "\n",
        "The data may contain incorrect or missing values and other artifacts. Moreover, types may not match the content. In order to proceed, we must find and correct all errors. In addition, it is necessary to individually explore every features distribution. \n",
        "\n",
        "Based on the research results and some logical considerations, we should decide how to use those features: remove, leave, replace continuous values with classes or categories, reduce the existing number of categories, apply a mathematical function to the values, and so on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bcaNuvvCIcs",
        "colab_type": "text"
      },
      "source": [
        "<a id='category'></a>\n",
        "#### Category"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYWts6IjCIct",
        "colab_type": "text"
      },
      "source": [
        "Let's look at the distribution of applications by category using the *value_counts()* method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TgwO0rMCIct",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.Category.value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5m1jCWhCIcv",
        "colab_type": "text"
      },
      "source": [
        "Most applications are in FAMILY, GAME and TOOLS  categories."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guvmJVO8CIcw",
        "colab_type": "text"
      },
      "source": [
        "There is only one application in  \"2\" category. In addition, this name does not make sense in this context.\n",
        "\n",
        "Let's take a look at this app:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFq58Ee1CIcx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df[df.Category == '2']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIDMatA5CIc1",
        "colab_type": "text"
      },
      "source": [
        "There was a shift of a row on one column to the left. Let's delete this sample."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tl74RpQZCIc2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df[df.Category != '2']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyunQ1ldCIc6",
        "colab_type": "text"
      },
      "source": [
        "**Task 2:** Are there apps with missing category values? You can use methods: [isnull](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.isnull.html) and  [sum](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sum.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oryOlsk4CIc8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# write your code here\n",
        "df.Category.isnull().sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "espY119ECIdC",
        "colab_type": "text"
      },
      "source": [
        "[Table of contents](#0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzeRDWeOCIdE",
        "colab_type": "text"
      },
      "source": [
        "<a id='rating'></a>\n",
        "#### Rating"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6flRuZ4CIdF",
        "colab_type": "text"
      },
      "source": [
        "Rating - is our target column. It's important to check that all the values are alright"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3DF-PRGCIdG",
        "colab_type": "text"
      },
      "source": [
        "**Task 3** What rating does the majority of apps have?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-ZNlzPhCIdI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# write your code here\n",
        "df.Rating.value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-t9wQAWCIdK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(7,7))\n",
        "plt.pie(df.Rating.value_counts(),\n",
        "        labels=df.Rating.value_counts().index,\n",
        "        autopct='%1.1f%%',\n",
        "        startangle=120, \n",
        "        explode=[0.02]*3)\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47zlLC_XCIdM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.Rating.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWbzEjs9CIdN",
        "colab_type": "text"
      },
      "source": [
        "Checking for missing values:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwx_jpjTCIdO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.Rating.isnull().sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a78hpMuKCIdP",
        "colab_type": "text"
      },
      "source": [
        "[Table of contents](#0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqN3bi-LCIdQ",
        "colab_type": "text"
      },
      "source": [
        "<a id='size'></a>\n",
        "#### Size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZa2EipeCIdQ",
        "colab_type": "text"
      },
      "source": [
        "The information contained in Size is clearly numeric, however the column is categorical because it contains special characters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96cf59NWCIdR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.Size.value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFNxCrwQCIdS",
        "colab_type": "text"
      },
      "source": [
        "The size of applications is specified in megabytes and kilobytes. But the most common is the value 'Varies with device'.\n",
        "\n",
        "Because of the characters M (megabytes) and K (kilobytes), *pandas* will not be able to automatically translate them into numbers and will throw an error. So, it's not possible just change the column type \"Size\" from object to float using the astype('float') method. \n",
        "\n",
        "Let's transfrom all values to the amount of megabytes and then to the number format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlGYiIcvCIdT",
        "colab_type": "text"
      },
      "source": [
        "**Task 4** Write a function that will receive one of the possible values of the Size column to the input, and will return either the corresponding number of megabytes or np.NaN. Use function [string.replace()](https://www.geeksforgeeks.org/python-string-replace/). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYfJp1ydCIdU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def check_size(size_value):\n",
        "#  write your code here  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mlF6hc_CIdV",
        "colab_type": "text"
      },
      "source": [
        "Now let's apply our function to the Size column:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YEsgq3aCIdW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.Size = df.Size.apply(check_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4g_cyAV5CIdX",
        "colab_type": "text"
      },
      "source": [
        "Let's check that the column is in the float format now:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEtryQRTCIdY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.Size.dtype"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vy0xwkmwCIda",
        "colab_type": "text"
      },
      "source": [
        "All values 'Variations with device' are missing now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZG1w8baCIdc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.Size.isnull().sum()/len(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6Pq1uVFCIde",
        "colab_type": "text"
      },
      "source": [
        "As you can see there is ~16% of the samples with NaN values (previously 'Variations with device'). That's quite a lot. To deal with this, we can create a new column with binary values that will store information about these NaN samples (perhaps this information will be useful) and then process them in the original column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDR24Q8vCIdf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['unknown_size'] = df.Size.isnull()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T33PA4AQCIdg",
        "colab_type": "text"
      },
      "source": [
        "Now let's handle the missing values. There are several traditional methods of dealing with them:\n",
        "\n",
        "1. remove\n",
        "2. replace with average, mode, median, 0\n",
        "3. replace with nearby examples (mostly for time series data)\n",
        "4. use the distribution of existing data and use it to generate new values for features. (useful when using linear models)\n",
        "5. make this column a target variable and train a separate model to predict the missing values\n",
        "\n",
        "The easiest option is to delete these rows. But 16% are a lot of samples. We don't want to lose so much useful information.\n",
        "The best way is to use a special model, but it can take a lot of time.\n",
        "\n",
        "Usually it's better to compare different approaches."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hG_Ti9qCIdh",
        "colab_type": "text"
      },
      "source": [
        "For this case we will use the median, but you can try experimenting with other options."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxRRkhFeCIdi",
        "colab_type": "text"
      },
      "source": [
        "**Task 5** Fill in the missing values in the Size column with the median value of the same column. Use the methods [fillna](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html) и [median](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.median.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ag-lyHMTCIdi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# write your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zS69WjpyCIdk",
        "colab_type": "text"
      },
      "source": [
        "Just in case, check that there are no empty values:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afV6XdCuCIdk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.Size.isnull().sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tf707i75CIdm",
        "colab_type": "text"
      },
      "source": [
        "[Table of contents](#0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eBROlVcCIdn",
        "colab_type": "text"
      },
      "source": [
        "<a id='type'></a>\n",
        "#### Type"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3-9v2TGCIdo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.Type.value_counts(normalize=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYaSEHqxCIdp",
        "colab_type": "text"
      },
      "source": [
        "93% of apps are free. To say more, you need to look at the relationships with other variables. We will deal with this in the next section. Let's check for missing values:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvoKVpAGCIdq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.Type.isnull().sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtoKfLpPCIdt",
        "colab_type": "text"
      },
      "source": [
        "There is one missing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeWvMKD9CIdu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df[df.Type.isnull()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82uZy-7hCIdv",
        "colab_type": "text"
      },
      "source": [
        "**Task 6** Handle the instance with the missing \"Type\" in the best way on your opinion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "847v3q0qCIdw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# write your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWa4C5YuCIdx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.Type.isnull().sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIEIZSotCIdy",
        "colab_type": "text"
      },
      "source": [
        "[Table of contents](#0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFF1mz12CIdz",
        "colab_type": "text"
      },
      "source": [
        "<a id='price'></a>\n",
        "#### Price"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxneW9fiCIdz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.Price.value_counts()[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPdKi0RjCId0",
        "colab_type": "text"
      },
      "source": [
        "The variable is numeric, but it contains special characters. That's why it has 'object' Type."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEnjHARsCId1",
        "colab_type": "text"
      },
      "source": [
        "**Task 7:** Remove the special character \"$\". Change the column type to 'float' using the method [astype()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.astype.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9y5xQoQuCId1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# write your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMVKMH_aCId5",
        "colab_type": "text"
      },
      "source": [
        "Let's create a distribution plot of prices among the paid apps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXwrEytxCId6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=[15, 7])\n",
        "sns.distplot(df.Price[df.Type == 'Paid'])\n",
        "plt.xlabel(\"Price, $\")\n",
        "plt.title('The distribution of apps by price' ,size = 24)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoQIjT1yCId7",
        "colab_type": "text"
      },
      "source": [
        "Cheap apps make up the most part. Moreover, the rest of apps take too small part of all apps, so it is difficult to say anything by the graphic. \n",
        "Let's look at logarithm of price."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByTQEsldCId8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=[15, 7])\n",
        "sns.distplot(np.log(df.Price[df.Type == 'Paid']))\n",
        "plt.xlabel(\"log(Price), $\")\n",
        "plt.title('The distribution of apps by price',size = 24)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEOJnBa_CId-",
        "colab_type": "text"
      },
      "source": [
        "Most apps cost around 1 and 3 dollars. The strong discreteness of small values is related to the logarithmic scale. Strange peak at large values of the price is interesting. Let's have a look."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3aG6JfrCId_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df[df.Price > 200]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Buq_u4PHCIeA",
        "colab_type": "text"
      },
      "source": [
        "Strange apps for the rich. Perhaps they are an indicator of status in certain circles or something like that.\n",
        "Let's check for missing values and move on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NpwUpNQCIeB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.Price.isnull().sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Op8M-csvCIeC",
        "colab_type": "text"
      },
      "source": [
        "[Table of contents](#0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFhHM8a5CIeC",
        "colab_type": "text"
      },
      "source": [
        "<a id='content_rating'></a>\n",
        "#### Content Rating"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdlh0rv3CIeD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['Content Rating'].value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ms7j12U2CIeE",
        "colab_type": "text"
      },
      "source": [
        "Remove the \"Adults only 18+\" and \"Unrated classes\". They are too few."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0PcTovrCIeE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df[(df['Content Rating'] != 'Adults only 18+') & (df['Content Rating']!= 'Unrated')] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wa-ua3p0CIeI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['Content Rating'].value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JdwMNmoCIeJ",
        "colab_type": "text"
      },
      "source": [
        "Check for missing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTInziz8CIeK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['Content Rating'].isnull().sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbbQdQPuCIeP",
        "colab_type": "text"
      },
      "source": [
        "[Table of contents](#0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZISTOZ17CIeP",
        "colab_type": "text"
      },
      "source": [
        "<a id='genres'></a>\n",
        "#### Genres"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfI0xFdfCIeQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.Genres.value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WP83joNJCIeR",
        "colab_type": "text"
      },
      "source": [
        "THere are quite a lot of genres - 119. Moreover, some of them are actually a result of combining two basic genres. Let's find out the amount of unique ones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIDDYy2fCIeS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "list_of_genres = []\n",
        "for i in df.Genres.str.split(';').values:\n",
        "    list_of_genres.extend(i)\n",
        "print('Amount of subcategory: {}'.format(len(set(list_of_genres))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkH0vA9KCIeT",
        "colab_type": "text"
      },
      "source": [
        "**Task 8** Calculate the amount of genres for each app and estimate the general distribution for them? Use [series.str.findall()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.findall.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3n7IQ2lSCIeT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# count_of_genres = ?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbd8CmUvCIeU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "counts_of_genres.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COHGfZrhCIeW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(counts_of_genres>1).sum()/len(counts_of_genres)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PM6_wv2CIeX",
        "colab_type": "text"
      },
      "source": [
        "There isn't a lot of applications with many genres. Let's look at some of them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLkfWy-KCIeX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df[counts_of_genres>1].head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ha4tHbnqCIeZ",
        "colab_type": "text"
      },
      "source": [
        "You may notice that the genre column often contains the same as the category column. We will explore this in more detail later. Now let's check for missing values and move on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tshlQrprCIeZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.Genres.isnull().sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Gi8iWVHCIeb",
        "colab_type": "text"
      },
      "source": [
        "[Table of contents](#0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DZWcQzuCIeb",
        "colab_type": "text"
      },
      "source": [
        "<a id='current_ver'></a>\n",
        "#### Current Version"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQqGgR5gCIeb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['Current Ver'].value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9ba9d7SCIec",
        "colab_type": "text"
      },
      "source": [
        "Because of too many unique values, this isn't a particularly informative column. It seems that each developer uses its own notation, so we will just remove it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5wYaqkuCIed",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df.drop(columns=['Current Ver'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bEFp6lUCIeh",
        "colab_type": "text"
      },
      "source": [
        "[Table of contents](#0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFgOyngACIeh",
        "colab_type": "text"
      },
      "source": [
        "<a id='android_ver'></a>\n",
        "#### Android Version"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJk_QuKFCIej",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['Android Ver'].value_counts(normalize=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mghdbQg-CIek",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['Android Ver'].isnull().sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mKkGuo-CIem",
        "colab_type": "text"
      },
      "source": [
        "There are 2 missing values. Remove it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrzrGBbQCIem",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df.dropna(subset=['Android Ver'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AczntncYCIen",
        "colab_type": "text"
      },
      "source": [
        "Let's scale up the classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAH4LGdpCIen",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(1, 9):\n",
        "    df.loc[df['Android Ver'].str.contains('^{}..*'.format(i)), 'Android Ver'] = '{} and up'.format(i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmZ8xE8MCIeq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['Android Ver'].value_counts(normalize=True).sort_index()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcVQpGmlCIes",
        "colab_type": "text"
      },
      "source": [
        "Groups 1-3 and 5-8 still have a small number of examples. The biggest one is 4 group. We probably should scale up the classes even more and combine 1-3 and 5-8 into two separate groups."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39DZuOuoCIet",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.loc[df['Android Ver'].str.contains('^[123]..*'), 'Android Ver'] = '1 and up'\n",
        "df.loc[df['Android Ver'].str.contains('^[5678]..*'), 'Android Ver'] = '5 and up'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OV6rthtCCIeu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['Android Ver'].value_counts(normalize=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQQLpKIKCIev",
        "colab_type": "text"
      },
      "source": [
        "With such distribution of values, this feature can actually contribute to the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHbKZyOtCIew",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['Android Ver'].isnull().sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsWIx8ulCIey",
        "colab_type": "text"
      },
      "source": [
        "[Table of contents](#0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smTaHCNDCIez",
        "colab_type": "text"
      },
      "source": [
        "<a id='final_check'></a>\n",
        "#### Final check"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQI2NNXfCIez",
        "colab_type": "text"
      },
      "source": [
        "Сheck if there are missing values in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxAwOxa1CIe0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.isnull().sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YrKoManCIe1",
        "colab_type": "text"
      },
      "source": [
        "<a id='removing_duplicates'></a>\n",
        "### Removing duplicates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCB8pXpVCIe2",
        "colab_type": "text"
      },
      "source": [
        "Now we should make sure there are no applications that occur in the dataset more than once, or in other words - duplicate each other."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0wjquanCIe2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_dup = df[df.duplicated(subset='App')]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vren3Cj0CIe3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_dup.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2jHzhTnCIe4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dup_apps = ['Box', 'Call Blocker', 'Bubble Shooter', 'Word Search']\n",
        "\n",
        "df_tmp = pd.DataFrame()\n",
        "for col in dup_apps:\n",
        "    df_tmp = pd.concat((df_tmp, df[df.App == col]))\n",
        "df_tmp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hQsQoG4CIe5",
        "colab_type": "text"
      },
      "source": [
        "Some applications have duplicates with different sizes, so we’ll sort by size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eh4vjxn3CIe7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df.sort_values('Size')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzBhecpwCIe8",
        "colab_type": "text"
      },
      "source": [
        "**Task 9** Remove duplicates with [drop_duplicates()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop_duplicates.html) (keep applications with the largest size)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGkeWA9xCIe8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# write your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75u1FLX5CIe9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUGcmeNlCIfA",
        "colab_type": "text"
      },
      "source": [
        "After filtering, we lost ~11% from the total volume of our dataset. Obviously this is an unpleasant measure, but it has to be done."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YztVlsqCIfA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_4_12MCCIfB",
        "colab_type": "text"
      },
      "source": [
        "[Table of contents](#0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtLzmXhHCIfB",
        "colab_type": "text"
      },
      "source": [
        "<a id='2_data_relations'></a>\n",
        "## 2. Exploring the relationship between variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHqMym_NCIfD",
        "colab_type": "text"
      },
      "source": [
        "<a id='numerical_features'></a>\n",
        "###  Numerical features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlLOz-KPCIfD",
        "colab_type": "text"
      },
      "source": [
        "One of the simple and effective methods to explore the relationship between numerical features is correlation analisys. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdfJMaKECIfE",
        "colab_type": "text"
      },
      "source": [
        "> **Correlation**  - is a statistical relationship between two or more random variables (or variables that can be considered as random with some acceptable degree of accuracy). Changes in one or more values lead to a systematic changing of another values.\n",
        "\n",
        "A correlation matrix is a matrix that contains the values of correlations between variables. In our case the correlation values between all pairs of numerical features will be obtained.\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/0/02/Correlation_examples.png\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Le-yrmacCIfE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corr_df = df.corr()\n",
        "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
        "mask = np.zeros_like(corr_df, dtype=np.bool)\n",
        "mask[np.triu_indices_from(mask)] = True\n",
        "plt.subplots(figsize=[15,10])\n",
        "plt.title('Correlation matrix')\n",
        "sns.heatmap(corr_df, mask=mask, cmap=cmap, linewidths=.5, annot=True)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAcMkGf6CIfF",
        "colab_type": "text"
      },
      "source": [
        "+ There is no correlation between numeric variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9jjJ37ZCIfF",
        "colab_type": "text"
      },
      "source": [
        "As the next step we will use *pairplot* method for our numeric variables. In order to be able to tell something we shall consider *price* on a logarithmic scale. Let's make a separate dataframe for this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjxhqWSLCIfG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_log = df.copy()\n",
        "df_log['log_price'] = np.log1p(df_log.Price)\n",
        "df_log.unknown_size = df_log.unknown_size.astype(int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_33GCzOaCIfH",
        "colab_type": "text"
      },
      "source": [
        "*Pairplots* compare the distributions of variables in pairs and allow you to make the most common assumptions, which can then be checked and clarified later.\n",
        " \n",
        "The distributions of the variables are shown on the diagonal with a color breakdown by the selected types. Relations between two variables are drawn above and below the diagonal: each point is one application in our case.\n",
        "\n",
        "By default, pairplots use KDE. But it is often built incorrectly - instead of smooth \"bells\" there are thin high peaks, ridges and other artefacts. Therefore, the using of histograms is more reliable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFhZod6iCIfI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ax = sns.pairplot(df_log, hue='Rating', \n",
        "                  vars = ['Size', 'log_price', 'unknown_size'],\n",
        "                  plot_kws = {'alpha': 0.6, 's': 80, 'edgecolor': 'w'},\n",
        "                  diag_kind='hist', diag_kws = {'edgecolor': 'w', 'alpha': 0.6}, \n",
        "                  size = 4)\n",
        "ax.fig.suptitle('Pairplot with grouping by rating', y=1.02, size=18)\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QusqpmVaCIfJ",
        "colab_type": "text"
      },
      "source": [
        "**Task 10:** Explore the graphics. What conclusions can you make?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4BVxdMgCIfJ",
        "colab_type": "text"
      },
      "source": [
        "[Table of contents](#0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUCEZr02CIfJ",
        "colab_type": "text"
      },
      "source": [
        "<a id='app_len_&_rating'></a>\n",
        "### Application length and rating"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwF4ytbECIfJ",
        "colab_type": "text"
      },
      "source": [
        "Let's see how the length of the application name affects the rating. To do this, create a separate dataframe with the App and Rating columns. As a result we will receive a column with the length of the application name."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58OxxZNCCIfK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_app = pd.DataFrame({'App': df.App,\n",
        "                       'Rating': df.Rating})\n",
        "df_app['App_len'] = df_app.App.apply(len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IowRFs0gCIfL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(15, 7))\n",
        "for i in sorted(df_app.Rating.unique()):\n",
        "    sns.kdeplot(df_app.App_len[df_app.Rating==i],\n",
        "                shade=True,\n",
        "                legend=False,)\n",
        "plt.legend(labels=sorted(df_app.Rating.unique()))\n",
        "plt.xlabel(\"Number of characters in App\")\n",
        "plt.title(\"The distribution of the Apps name\",size = 18)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SwckKBhCIfM",
        "colab_type": "text"
      },
      "source": [
        "Apps with \"0\" rating are more likely to have a short title. At the same time, more popular applications are in the range of 20 to 50 characters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19iVwI9VCIfM",
        "colab_type": "text"
      },
      "source": [
        "<a id='type_&_price'></a>\n",
        "### Price and Type"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwLTKQq5CIfN",
        "colab_type": "text"
      },
      "source": [
        "It is logical to assume that if \"Type\" is \"Free\", then \"Price\" should be equal to zero.\n",
        "\n",
        "Let's group \"Type\" and look at the average price in each group (\"Free\" and \"Paid\")."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2vLb6GpCIfN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.groupby('Type')['Price'].mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpjb0pInCIfO",
        "colab_type": "text"
      },
      "source": [
        "The average price of a \"Free\" app is zero dollars, a paid one is 14 dollars. Looks fine.\n",
        "\n",
        "[Table of contents](#0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKEyTGAWCIfO",
        "colab_type": "text"
      },
      "source": [
        "<a id='category_&_genres'></a>\n",
        "###  Category and Genres"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cN470lCwCIfP",
        "colab_type": "text"
      },
      "source": [
        "Genres is a subsection of Category."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Yfis07GCIfP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miy6YtrsCIfQ",
        "colab_type": "text"
      },
      "source": [
        "Let's see which categories use the most subcategories.\n",
        "Next we will aggregate categories and genres in a separate table and make a new column with the number of genres."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3L4x5awCIfQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_cat_genrs = df[['Category', 'Genres', 'Rating']]\n",
        "df_cat_genrs['Count_of_genres'] = df_cat_genrs.Genres.str.findall(';').apply(len)+1\n",
        "df_cat_genrs.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-NDvof4CIfS",
        "colab_type": "text"
      },
      "source": [
        "**Task 11** Explore the genre number statistics for each category. Use [groupby](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html) и [describe](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3u1Xz6YSCIfS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# write your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HZDNTRLCIfT",
        "colab_type": "text"
      },
      "source": [
        "The maximum number of subcategories in one category is 2, the minimum is 1. Two subcategories are most often found in the categories PARENTING, FAMILY, EDUCATION. Only 11 categories have multiple genres."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOGaRzRbCIfU",
        "colab_type": "text"
      },
      "source": [
        "Categories and genres often overlap. Let's study this point in details.\n",
        "\n",
        "For starters we should bring categories and genres to a common format: transform them into lower case (*str.lower()*) and remove special characters (replace the special characters with spaces or nothing using the *str.replace()*).\n",
        "\n",
        "\n",
        "_**Note:** pandas has many methods for working with strings. You can read more about them in the documentation_\n",
        "[Working with Text Data](https://pandas.pydata.org/pandas-docs/stable/user_guide/text.html).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSxSnevmCIfU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_cat_genrs.Category = df_cat_genrs.Category.str.lower()\n",
        "df_cat_genrs.Genres = df_cat_genrs.Genres.str.lower()\n",
        "df_cat_genrs.Genres = df_cat_genrs.Genres.str.replace('&', '')\n",
        "df_cat_genrs.Genres = df_cat_genrs.Genres.str.replace(';', ' ')\n",
        "df_cat_genrs.Category = df_cat_genrs.Category.str.replace('_', ' ')\n",
        "df_cat_genrs.Category = df_cat_genrs.Category.str.replace('and', '')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImzuyRvVCIfV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_cat_genrs.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D16j-exLCIfW",
        "colab_type": "text"
      },
      "source": [
        "Let's calculate how many apps have the same genres and categories. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVcePrV8CIfW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(df_cat_genrs['Category'] == df_cat_genrs['Genres']).sum()/len(df_cat_genrs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBerOtCECIfZ",
        "colab_type": "text"
      },
      "source": [
        "The columns are completely the same for almost 70%. That is, 70% of the values don't contain new information. Most likely, there are more matches considering possible errors related to usage of regular expressions on raw data.\n",
        "\n",
        "Let's add a column whether the genre is equal to the category or not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7OX-wMrCIfa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_cat_genrs['is_cat_equal_genre'] = df_cat_genrs['Category'] == df_cat_genrs['Genres']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GkAmVIkCIfa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_cat_genrs.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8pakvyrCIfb",
        "colab_type": "text"
      },
      "source": [
        "**Task 12** Explore whether there is a difference between distributions of apps with the same and with different categories and genres (use [sns.countplot](https://seaborn.pydata.org/generated/seaborn.countplot.html))."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fsn2vPmhCIfc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# write your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8ExBlb1CIfd",
        "colab_type": "text"
      },
      "source": [
        "The rating distributions for True and False are slightly different. You can see that applications with a rating 0 stand out a little bit. To numerically estimate this distribution, we will use the contingency table."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxlguZy-CIfd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cr_tab = pd.crosstab(df_cat_genrs.Rating, df_cat_genrs.is_cat_equal_genre)\n",
        "cr_tab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZGbbXkvCIfe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cr_tab / cr_tab.min()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExizQ2nYCIff",
        "colab_type": "text"
      },
      "source": [
        "The difference in distribution for apps with same category and genre is negligible. On the other side, applications with different genre and category are 1.5 times more likely to have \"2\" rating.\n",
        "\n",
        "Such column could be useful in predicting the rating of applications. At the same time there is little sense to leave two columns with almost the same information. That's why we remove the column genre and add column is_cat_equal_genre in the main dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHfinBG-CIff",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df.drop(columns=['Genres'])\n",
        "df['is_cat_equal_genre'] = df_cat_genrs['is_cat_equal_genre'] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AP1zzrdHCIfh",
        "colab_type": "text"
      },
      "source": [
        "[Table of contents](#0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oyeRqBYCIfh",
        "colab_type": "text"
      },
      "source": [
        "<a id='rating_&_content_rating'></a>\n",
        "### Rating and Content Rating"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDBnsCMgCIfh",
        "colab_type": "text"
      },
      "source": [
        "**Task 13** Explore the relationship between rating and content rating columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JW5-n5awCIfh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# write your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ovi4iMFUCIfj",
        "colab_type": "text"
      },
      "source": [
        "Let's look at the table of both at the rating contingency with Content Rating column and the is_not_equal_genre column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEqD4qtkCIfj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd_ct_comp = pd.crosstab(df.Rating, [df['Content Rating'], df.is_cat_equal_genre])\n",
        "pd_ct_comp /= pd_ct_comp.sum()\n",
        "pd_ct_comp = pd_ct_comp.style.background_gradient(cmap='summer_r')\n",
        "pd_ct_comp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMQTwTd4CIfk",
        "colab_type": "text"
      },
      "source": [
        "First thing you could notice are applications with the age rating \"Everyone\" and the same categories and genres. For those application the most common rating is \"0\". Apps with Everyone 10+ and is_not_equal_genre False  mostly have rating \"2\".  \n",
        "The most interesting thing is that the distribution of the rating for Teen and Mature 17+ applications  are fairly the same in general, they have a different distribution for applications with the same genre and category (*is_cat_equal_genre* True)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBfnS4phCIfk",
        "colab_type": "text"
      },
      "source": [
        "<a id='category_&_price'></a>\n",
        "### Category and Price"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIt_amQ3CIfk",
        "colab_type": "text"
      },
      "source": [
        "Another convenient way to explore the data distribution is a *catplot*. In its simplest, it allows you to display each individual example in an individual column. Thus, you can get a sense the of the data density. \n",
        "\n",
        "We will use the previously created logarithmic price column and consider only paid applications and categories with at least 10 such applications."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2xGd6xNCIfl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "g = sns.catplot(x=\"Category\",y=\"log_price\", data=df_log[df_log.Type=='Paid'])\n",
        "g.fig.set_figheight(7)\n",
        "g.fig.set_figwidth(20)\n",
        "plt.title('The distribution of logarithmic prie in different categories', size = 20)\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRqPIu3vCIfl",
        "colab_type": "text"
      },
      "source": [
        "There are not a lot of paid applications, but numerous categories, so the graphics are so sparse. You can see that some categories have outlier applications: for instance, Communication category applications are mostly grouped together but one stands out from the others. Finance and Lifestyle are also quite sparse. Note, that the Family group contains a group of detached expensive applications.\n",
        "\n",
        "Now will take a look at the same distributions, but using confidence intervals.\n",
        ">**Confidence Intervals** - is a type of interval estimate used in statistics that are calculated for a given level of significance. They allow us to make a statement that the true value of an unknown statistical parameter of the general population is in the obtained range of values with a probability that is given by the selected level of statistical significance.\n",
        "\n",
        ">What is the practical meaning of the confidence interval?\n",
        "\n",
        ">+ A wide confidence interval indicates that the sample average doesn't accurately approximate the general average. This is usually related to an insufficient sample size, or to its heterogeneity, i.e. the large variance. Both give a large average error and, correspondingly, a wide CI. This is the reason for returning to the planning stage of the study. \n",
        "\n",
        ">+ The upper and lower limits of the CI give an estimation of whether the results will be statistically significant or not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFpo5njECIfm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "g = sns.catplot(x=\"Category\",y=\"log_price\", data=df_log[df_log.Type=='Paid'], kind='bar')\n",
        "g.fig.set_figheight(7)\n",
        "g.fig.set_figwidth(20)\n",
        "plt.title('The distribution of logarithmic price in different categories', size = 20)\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tl9h_c7eCIfm",
        "colab_type": "text"
      },
      "source": [
        "In this graph, the column height shows the average value, and the bar shows the confidence interval. You can notice that the Finance and Lifestyle categories have a very wide confidence interval. This means that these categories are highly sparse and have some isolated groups in different parts of the distributions. It is not valid to use the average value for such categories. There is no sense to consider other categories with a wide confidence interval - there are too few examples to consider the stats meaningful.\n",
        "The Family category has a narrow confidence interval: although there are the separate group of expensive applications, their number is small and the distribution of prices in this category is uniform in general.\n",
        "\n",
        "Let's look at groups of expensive applications in the categories Finance and Lifestyle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnAZ64ZYCIfn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_log.log_price[(df_log.Category.isin(['LIFESTYLE', 'FINANCE'])) & (df_log.Type == 'Paid')].describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrBKFX88CIfo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_log[(df_log.log_price > 5.) & (df_log.Category.isin(['LIFESTYLE', 'FINANCE']))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCx_J7MrCIfp",
        "colab_type": "text"
      },
      "source": [
        "The shift was due to a few expensive \"freaky\" apps we already saw earlier. Let's remove them and rearrange the graphic."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mms9um-LCIfp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "g = sns.catplot(x=\"Category\",y=\"log_price\", data=df_log[(df_log.Type=='Paid') & (df_log.log_price < 5)], kind='bar')\n",
        "g.fig.set_figheight(7)\n",
        "g.fig.set_figwidth(20)\n",
        "plt.title('The distribution of logarithmic price in different categories', size = 20)\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyQxDHZ7CIfq",
        "colab_type": "text"
      },
      "source": [
        "The Lifestyle and Finance categories became more realistic and stable. Let's see if this affects the rating of applications in these categories. To do so, we will select cheap and expensive samples from these groups and place them in separate dataframes, add a distinctive column log_prices and then combine them into a common table.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIT-5z5GCIfq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_lf = df_log[(df_log.Category.isin(['LIFESTYLE','FINANCE'])) & (df_log.Type == 'Paid')].assign(log_prices='All')\n",
        "df_lf2 = df_lf[df_lf.log_price <5.].assign(log_prices='< 5')\n",
        "df_lf3 = df_lf[df_lf.log_price >=5.].assign(log_prices='> 5')\n",
        "cdf = pd.concat([df_lf, df_lf2, df_lf3], ignore_index=True)      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBtwFzz8CIfr",
        "colab_type": "text"
      },
      "source": [
        "Let's build a boxplot for each group."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plIUgnElCIfr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(15,5))\n",
        "ax = sns.boxplot(y=\"log_prices\", x=\"log_price\", hue=\"Rating\", data=cdf)  # RUN PLOT   \n",
        "plt.show()\n",
        "\n",
        "plt.clf()\n",
        "plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRuucaS5CIfs",
        "colab_type": "text"
      },
      "source": [
        "There is just 1 application with a rating of 2 among the paid applications in the categories Lifestyle and Finance, so there is only a dash. For ratings 0 and 1 we can notice that expensive applications mess up the statistics. Without them (removing these applications) it would be easier to identify the rating of the application from the remaining distributions.\n",
        "\n",
        "For this reason, such applications should either be removed or put separate into a category (if there is a sufficient amount), as a result clearing the original categories. In our case it is easier to remove them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZxJe6YNCIfs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df[~((df_log.Category.isin(['LIFESTYLE', 'FINANCE'])) & (df_log.log_price > 5))]\n",
        "df_log = df_log[~((df_log.Category.isin(['LIFESTYLE', 'FINANCE'])) & (df_log.log_price > 5))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phvBKUw0CIft",
        "colab_type": "text"
      },
      "source": [
        "<a id='Категории_и_размер'></a>\n",
        "### Category and Size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eWPQcPSCIft",
        "colab_type": "text"
      },
      "source": [
        "Let's add the logarithm of size to df_log."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bH7d5xKCIft",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_log['log_size'] = df_log.Size.apply(np.log1p)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhyNxndwCIfu",
        "colab_type": "text"
      },
      "source": [
        "**Task 14** Explore the distribution of app size in each category. Use the df_log table and the log_size column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZ-F0XW3CIfu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# write your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UWKJ2HBCIfv",
        "colab_type": "text"
      },
      "source": [
        "[Table of contents](#0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JqqYo1YCIfy",
        "colab_type": "text"
      },
      "source": [
        "<a id='3_feature_space'></a>\n",
        "## 3. Working with feature space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k42iHr18CIfy",
        "colab_type": "text"
      },
      "source": [
        "Let's create a table where we will store the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-TLBWHWCIf0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results = pd.DataFrame(columns=['method', 'model', 'val score', 'test score', 'learning time', 'predict time'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80N1FBscCIf8",
        "colab_type": "text"
      },
      "source": [
        "To measure processing time we will use a special context manager."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHgRxEQyCIf8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Timer(object):\n",
        "    def __init__(self):\n",
        "        self.elapsed_time = 0\n",
        "    def __enter__(self):\n",
        "        self.start = time()\n",
        "    def __exit__(self, type, value, traceback):\n",
        "        self.end = time()\n",
        "        self.elapsed_time = int((self.end - self.start)*1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Msri4vXkCIf9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "timer = Timer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GR8PQxf8CIf-",
        "colab_type": "text"
      },
      "source": [
        "<a id='base_model'></a>\n",
        "### Base model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igCkYld7CIf_",
        "colab_type": "text"
      },
      "source": [
        "Our task is to determine the rating of the application. Let's take this column into a separate variable, and remove it from the main dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSQAQuAiCIf_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y = df.Rating.astype(int)\n",
        "df = df.drop(columns=['Rating'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ei4JYXoeCIgA",
        "colab_type": "text"
      },
      "source": [
        "The so-called *majority classifier* can be the simplest type of a base model. The point is: we always predict the class that is most common in our training dataset.\n",
        "\n",
        "Let's split the data into training and test sets. For appropriate comparision of the different models, this partition must always be the same, that's why we use fixed *random_state*. For now, we don't need the features, only the target variable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsgEZmd1CIgB",
        "colab_type": "text"
      },
      "source": [
        "We remind you that the train/test split of dataset samples should actually be performed before data processing. ([Attention])(#attention)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCEsfW6RCIgB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train, y_test = train_test_split(Y,  test_size=0.3, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_vfGCT7CIgB",
        "colab_type": "text"
      },
      "source": [
        "Now let's determine which class is most common in the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13vXQwk7CIgC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classes, counts = np.unique(y_train, return_counts=True)\n",
        "major_class = classes[np.argmax(counts)]\n",
        "major_class"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rT4MgkNCIgC",
        "colab_type": "text"
      },
      "source": [
        "In the training set, samples with second class are more common. Then our test prediction will be an array with the same size as the size of out test set and it will be completely filled with 2. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4816k6nCIgD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_predict = np.full(y_test.shape, major_class)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7q_l9sWiCIgE",
        "colab_type": "text"
      },
      "source": [
        "To analyze the results, we will use a function that displays several different metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riwlXK7wCIgE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_metrics(y_predict, y_test):\n",
        "    print('acсuracy: {:.4f}'.format(accuracy_score(y_predict, y_test)))\n",
        "    print('F1 score: {:.4f}'.format(f1_score(y_predict, y_test, average='macro')))\n",
        "    print(classification_report(y_predict, y_test))\n",
        "    print(confusion_matrix(y_predict, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuRd23ohCIgE",
        "colab_type": "text"
      },
      "source": [
        "Now let's calculate the accuracy of the majority classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2U4p_lQCIgF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "calculate_metrics(base_predict, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9JMHWKZCIgG",
        "colab_type": "text"
      },
      "source": [
        "These will be our reference values. Let's write them in the table."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5nYHxmqCIgG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results = results.append({'method':'Baseline',\n",
        "                          'model': 'Majority',\n",
        "                          'val score': None,\n",
        "                          'test score': f1_score(base_predict, y_test, average='macro'), \n",
        "                          'learning time' : None,\n",
        "                          'predict time': None},\n",
        "                          ignore_index=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUp34F1YCIgG",
        "colab_type": "text"
      },
      "source": [
        "<a id='origin_features'></a>\n",
        "### Basic features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMRViQMtCIgH",
        "colab_type": "text"
      },
      "source": [
        "Now we are going to use the original feature space to predict the target variable and measure the accuracy of the prediction. We will use only numeric and boolean features: they don't require the preparation, unlike text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-WgzQn5CIgH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mc79InqXCIgI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "numeric_df = df.select_dtypes(include=['int64', 'float64', 'bool'])\n",
        "numeric_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndiygjwACIgJ",
        "colab_type": "text"
      },
      "source": [
        "Now we'll get our test and training sets. (notice that we use the same random_state and therefore the data is split the same way as before)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5eQOjv1CIgJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(numeric_df, Y, \n",
        "                                                    test_size=0.3, \n",
        "                                                    random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WlUz_i_CIgL",
        "colab_type": "text"
      },
      "source": [
        "We will train a Logistic Regression model - one of the simplest ones among linear classifiers. To perform hyperparameter optimization we'll use cross-validation by applying Pipeline and GridSearchCV functions. This specific classifier was chosen in order to decrease the training time within the workshop. Results for more complex models will be provided in the bonus file.\n",
        "\n",
        "> **Data scaling** — the process of adjusting the data to a single scale. The main methods are **normalization** - adjusting all features to a value in the range from 0 to 1, and **standardization** - data preprocessing, after which each feature has an average of 0 and a variance of 1\n",
        "\n",
        "Scaling the data can significantly affect the process of training a model. For example, it increases the speed and the stability of gradient descent algorithm. Moreover, methods that rely on working in a multidimensional space (such as KNN) will not work correctly if the features have a different scale. These effects don't always appear, but scaling the data is unlikely to make the results worse, so you should always perform it before training the model.\n",
        "\n",
        "No, we shall create a pipeline and set a grid of the hyperparameters for the classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gn22oeGjCIgL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pipe = Pipeline([('scale', StandardScaler()),\n",
        "                 ('clf', LogisticRegression(random_state=42))])\n",
        "\n",
        "params = {\n",
        "    'clf__C': [0.01, 0.05, 0.1, 0.5, 0.9, 0.99],\n",
        "    'clf__penalty': ['l1', 'l2']\n",
        "} \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMSqWjAXCIgM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(123)\n",
        "\n",
        "clf = GridSearchCV(pipe, \n",
        "                   cv=3,  \n",
        "                   param_grid=params, \n",
        "                   scoring='f1_macro',\n",
        "                   verbose=1,\n",
        "                   n_jobs=6)\n",
        "\n",
        "with timer:\n",
        "    clf.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ds1YbotyCIgM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_time = timer.elapsed_time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRlLI7AzCIgN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_clf = clf.best_estimator_.steps[1][1]\n",
        "best_clf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RL4Ku__FCIgO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clf.best_score_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MH7TWA5nCIgO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with timer:\n",
        "    predict = clf.predict(X_test)\n",
        "    \n",
        "predict_time = timer.elapsed_time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBn4df0DCIgP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "calculate_metrics(predict, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PcrugfDCIgQ",
        "colab_type": "text"
      },
      "source": [
        "Again, we shall save the results in the table."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PyQwwhQCIgR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results = results.append({'method':'Numeric',\n",
        "                          'model': 'LR',\n",
        "                          'val score': clf.best_score_,\n",
        "                          'test score': f1_score(clf.predict(X_test), y_test, average='macro'), \n",
        "                          'learning time': learning_time,\n",
        "                          'predict time': predict_time},\n",
        "                          ignore_index=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pP21R4-ECIgS",
        "colab_type": "text"
      },
      "source": [
        "As you can see, the F1-score of the model is slightly higher than the score of the majority classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1zOGu8QCIgS",
        "colab_type": "text"
      },
      "source": [
        "As the next step we should add categorical features. First we will encode them features into numerical ones. This can be done in multiple ways.\n",
        "\n",
        "For example, you could take one column, index all unique category names with numbers and just replace them in the column. This type of encoding is called Label Encoding. This method is usually bad because most of the time the values of numbers don't reflect the category relation. For example, encoding the application with type GAME as 1 and with MEDICAL as 2 turns out that MEDICAL > GAME, while such a comparison is incorrect. Therefore, many methods, especially linear ones, will not work well with this type of encoding. At the same time, methods based on decision trees will work fine.\n",
        "\n",
        "Another popular type of encoding is called Dummy Encoding or One-hot Encoding. Each unique category name has its own column. The values in this column can only be 0 and 1. If the sample instance has some category, then the corresponding dummy-column will contain a \"1\" value, and all others will be filled with zeros. This type of encoding has the disadvantage that if there are a lot of unique categories, the size of the feature matrix becomes too large and sparse. Therefore additional preprocessing of categorical data may be necessary.\n",
        "\n",
        "There are other, less popular and well known techniques, but here we will stick to One-hot encoding.\n",
        "\n",
        "First of all, we need to copy all the data to the new dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIYcsWM6CIgT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_dummies = df.copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEsnZoLOCIgT",
        "colab_type": "text"
      },
      "source": [
        "Next, it is necessary to delete the application names - they are unique for each sample. If we encode them with dummy encoding, it will lead to adding N, where N is the length of the whole dataset, and all these columns will have only one value 1 and all the others 0. Such features obviously do not work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FaGrmCiCIgT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_dummies = df_dummies.drop(columns=['App'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2LcrH90CIgU",
        "colab_type": "text"
      },
      "source": [
        "As was said before, sometimes it makes sense to encode columns with LabelEncoding. In our case the Content Rating column is suitable for this approach. We should encode it in a way, so with the growth of restrictions, the corresponding number will also grow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0IWBKBcCIgU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "con_rat_dict =  {'Everyone':0, 'Everyone 10+':1, 'Teen':2, 'Mature 17+':3}\n",
        "df_dummies = df_dummies.replace({\"Content Rating\": con_rat_dict})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cavHdtQ6CIgV",
        "colab_type": "text"
      },
      "source": [
        "Now let's encode the remaining categories with the dummy method. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkSE1QPOCIgV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_dummies = pd.get_dummies(df_dummies)\n",
        "\n",
        "df_dummies.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9Ucq14OCIgW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhOofBqiCIgY",
        "colab_type": "text"
      },
      "source": [
        "Now we have 44 features instead of 4.  \n",
        "Let's prepare a training and test set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPCS5lfvCIgY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df_dummies, Y,\n",
        "                                                    test_size=0.3, \n",
        "                                                    random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqfqGO0eCIgb",
        "colab_type": "text"
      },
      "source": [
        "**Task 15** Train the model, make a prediction on the test data and output the statistics. Don't forget to measure your training time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dB4OMnZCIgb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# write your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8FzFnOgCIgc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results = results.append({'method': 'Categories',\n",
        "                          'model': 'LR',\n",
        "                          'val score': clf.best_score_,\n",
        "                          'test score': f1_score(clf.predict(X_test), y_test, average='macro'), \n",
        "                          'learning time': learning_time,\n",
        "                          'predict time': predict_time},\n",
        "                          ignore_index=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OU9c8ZC0CIge",
        "colab_type": "text"
      },
      "source": [
        "As you can see, adding categorical features have improved the accuracy of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnELOKSzCIge",
        "colab_type": "text"
      },
      "source": [
        "[Table of contents](#0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aws1iPqZCIgf",
        "colab_type": "text"
      },
      "source": [
        "<a id='feature_generation'></a>\n",
        "### Feature generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_P3-F2QCIgf",
        "colab_type": "text"
      },
      "source": [
        "Now we will expand our feature space by adding new features - manually crafted with love and care! First we need a separate copy of the original dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daQSmnEaCIgf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_new = df.copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hr4pvcENCIgg",
        "colab_type": "text"
      },
      "source": [
        "Add price per megabite column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpkiZXa9CIgg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_new['price_for_mb'] = df_new.Size/df_new.Price\n",
        "# Заменим неопределенные значения, которые образовались при делении на ноль, нулём.\n",
        "df_new['price_for_mb'] = df_new['price_for_mb'].replace([np.inf, -np.inf], 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVj3VumjCIgh",
        "colab_type": "text"
      },
      "source": [
        "Also add logarithms of price and size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXfbBDCFCIgh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_new['log_price'] = df_new.Price.apply(np.log1p)\n",
        "df_new['log_size'] = df_new.Size.apply(np.log1p)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9Q4Qk4UCIgi",
        "colab_type": "text"
      },
      "source": [
        "Usually it is useful to indroduce polynomial features from numerical ones. Let's make them from the previously calculated logarithms of price and size. To do this, we can use the scikit-learn function Polynomial Features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrgT3euqCIgi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p = PolynomialFeatures(degree=2).fit(df_new[['log_price', 'log_size']])\n",
        "poly_df = pd.DataFrame(p.transform(df_new[['log_price', 'log_size']]), \n",
        "                        columns=p.get_feature_names(['log_price', 'log_size']))\n",
        "poly_df.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rw3z_NvsCIgi",
        "colab_type": "text"
      },
      "source": [
        "You can see that there are new columns low_price^2, log_price, log_size and log_size^2. Now you need to attach them to the main dataset, and discard unnecessary ones. To avoid errors during concatenation, we will replace the poly_df index with the df_new index."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6O3tWuVCIgi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "poly_df = poly_df.drop(columns=['log_price', 'log_size', '1'])\n",
        "poly_df.index = df_new.index\n",
        "df_new = pd.concat([df_new, poly_df.reindex(df_new.index)], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fe9kJ2DVCIgj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_new.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxf4Y2azCIgj",
        "colab_type": "text"
      },
      "source": [
        "Add the number of characters and the number of words in the title as features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWBuwQSYCIgk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_new['len_of_app_title'] = df_new.App.apply(len)\n",
        "df_new['count_of_app_title'] = df_new.App.str.split(' ').apply(len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76gGOd5TCIgl",
        "colab_type": "text"
      },
      "source": [
        "The names of applications contain a lot of garbage. It is better to clean it. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdkcHkBnCIgl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's change to lower case, remove special characters, leave only Latin letters and numbers.\n",
        "df_new['cleantext'] = df_new.App.str.lower()\n",
        "df_new['cleantext'] = df_new.cleantext.str.replace('[-_]', ' ')\n",
        "df_new['cleantext'] = df_new.cleantext.str.replace('[^0-9A-Za-z ]+', '')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnOJRzKLCIgm",
        "colab_type": "text"
      },
      "source": [
        "First we need to check how the text looks like after cleaning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fcr6QBnhCIgm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_new[['App', 'cleantext']].head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AU6yZUjXCIgm",
        "colab_type": "text"
      },
      "source": [
        "Now, after filtering, we can add the number of characters and the number of words as features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRniBHcdCIgn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_new['len_of_cleantext_title'] = df_new.cleantext.apply(len)\n",
        "df_new['count_of_cleantext_title'] = df_new.cleantext.str.split(' ').apply(len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J24hIEF-CIgn",
        "colab_type": "text"
      },
      "source": [
        "Next step is to add the difference between the original number of words and symbols and the normalized ones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3-GLTTECIgo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_new['diff_len_title'] = df_new['len_of_app_title'] - df_new['len_of_cleantext_title']\n",
        "df_new['diff_count_title'] = df_new['count_of_app_title'] - df_new['count_of_cleantext_title']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndhoEG-zCIgs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_new[['diff_len_title', 'diff_count_title']].describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBnb39MfCIgs",
        "colab_type": "text"
      },
      "source": [
        "For some reason there are normalized headers that have more words than original ones. We need to check it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GxOjawUCIgt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_new[['App', 'cleantext']][df_new['diff_count_title'] == -5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAEkFmUFCIgt",
        "colab_type": "text"
      },
      "source": [
        "Everything seems fine."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ht7YmfSgCIgt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len_vocab = len(set(' '.join(df_new.cleantext.tolist()).split(' ')))\n",
        "max_count_of_cleantext_title = round(df_new.count_of_cleantext_title.max(), 0)\n",
        "print('Number of unique words: {}'.format(len_vocab))\n",
        "print('Maximum number of words in a title: {}'.format(max_count_of_cleantext_title))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gQQQ29LCIgu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_new = df_new.drop(columns=['App', 'cleantext'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqN4k6owCIgv",
        "colab_type": "text"
      },
      "source": [
        "Let's encode categorical features into numerical vectors. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjAvRnULCIgv",
        "colab_type": "text"
      },
      "source": [
        "**Task 16:** Encode categorical features to one-hot format. Don't forget about the special case with Content Rating."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mor4AMVGCIgv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# write your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nH9zRZsCCIgw",
        "colab_type": "text"
      },
      "source": [
        "Now it's time to visualize our data. We will use UMAP for this.\n",
        "It is advisable to standardize the data before using this algorithm.\n",
        "\n",
        "\n",
        "> **UMAP** (Uniform Manifold Approximation and Projection) - this is a relatively new dimensionality reduction algorithm. Previously the most popular algorithm for these purposes was t-SNE. However UMAP is better in almost every way: UMAP has no restrictions on the dimension of the original feature space, it is much faster and more computationally efficient than t-SNE, and also better in the task of transferring the global data structure to a new, reduced space (as the authors of the algorithm claim). For these reasons t-SNE is hardly used these days.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OBHrZKpCIgw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scaled_data = StandardScaler().fit_transform(df_new)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ns0uIecxCIgx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "np.random.seed(1)\n",
        "X_umap = umap.UMAP(n_components=2, random_state=2).fit_transform(scaled_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02GgqZOnCIgy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=[15, 9])\n",
        "plt.title('UMAP')\n",
        "\n",
        "for i in Y.unique():\n",
        "    indx = np.where(Y.values == i)\n",
        "    plt.scatter(X_umap[indx, 0], X_umap[indx, 1], marker='.', alpha=0.5, label=i)\n",
        "    plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTr5Nl2xCIgy",
        "colab_type": "text"
      },
      "source": [
        "You can see that there is a lot of clusters and they contain different proportions of applications with different ratings. Let's apply a clustering a clustering algorithm to separate these groups. For now we will assume that we have 30 clusters and use the k_means algorithm. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWc5BsnQCIgy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kmeans_labels = cluster.KMeans(n_clusters=30).fit_predict(scaled_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srkxP5NPCIgz",
        "colab_type": "text"
      },
      "source": [
        "Now we will visualize the results of our clustering."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4E7xHKejCIgz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=[15, 9])\n",
        "ax = plt.subplot()\n",
        "\n",
        "for i in np.unique(kmeans_labels):\n",
        "    indx = np.where(kmeans_labels == i)\n",
        "    plt.scatter(X_umap[indx, 0], X_umap[indx, 1], marker='.', alpha=0.5, label=i)\n",
        "ax.legend(loc='upper center', bbox_to_anchor=(1.05, 1.0),  ncol=1, fancybox=True, shadow=True, fontsize=10)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIWjhg68CIg0",
        "colab_type": "text"
      },
      "source": [
        "Looks promissing. Let's explore whether there is a difference in the distributions of the target variable in each cluster. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZNsZH38CIg0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_clstr = pd.DataFrame({'Cluster':kmeans_labels, 'Rating':Y})\n",
        "g = sns.catplot(\"Rating\", col=\"Cluster\", col_wrap=5, data=df_clstr, kind=\"count\", height=2.5, aspect=.8, size=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RG96lRbYCIg0",
        "colab_type": "text"
      },
      "source": [
        "Obviously, the distribution of the target variable differs in many way throughout the clusters. We probably should add the cluster indexes to the main dataset. Note that the cluster index is a category. To perform one-hot encoding we will remove the target variable and then we can add new columns to the main dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwaqAudJCIg1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_clstr = df_clstr.drop(columns=['Rating'])\n",
        "df_clstr['Cluster'] = df_clstr['Cluster'].astype('category')\n",
        "df_clstr = pd.get_dummies(df_clstr)\n",
        "df_clstr.index = df_new.index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXpvYdfBCIg1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_new=pd.concat([df_new, df_clstr], axis=1)\n",
        "df_new.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1ZV8kV6CIg2",
        "colab_type": "text"
      },
      "source": [
        "It also would be possible to use aggregation features: for example, average, median, minimum, etc. for each category, age rating or other categorical features. You can apply simple mathematical operations to pairs of numerical features (for example, multiply the size of the application by the price): sometimes they don't make explicit sense, but they might be useful. There are also cases of logical addition of features (there is a category Medicine and an age rating of 18+, for example). To obtain such features, it is convenient to use the *featuretools* library. In general, the number of possible new features is just limited only by your imagination. Unfortunately today we don't have enough time go all crazy with manually creating these feature, so we will limit ourselves to the ones that we have already discussed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDFjI1o0CIg2",
        "colab_type": "text"
      },
      "source": [
        "Now let's try see if our work paid off"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8s1pKlCCIg2",
        "colab_type": "text"
      },
      "source": [
        "**Task 17:** Split the dataset and target variable into training and test set, train the model, make a prediction on the test data and output the statistics. Don't forget to measure your training time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n__LpXP2CIg2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# write your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poxZZTtvCIg3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results = results.append({'method': 'Manual features',\n",
        "                          'model': 'LR',\n",
        "                          'val score': clf.best_score_,\n",
        "                          'test score': f1_score(clf.predict(X_test), y_test, average='macro'), \n",
        "                          'learning time':learning_time,\n",
        "                          'predict time': predict_time},\n",
        "                          ignore_index=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sH0N6CNCIg3",
        "colab_type": "text"
      },
      "source": [
        "Adding the manually created features increased the effectiveness of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkQan6zbCIg3",
        "colab_type": "text"
      },
      "source": [
        "[Table of contents](#0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbmwAWzGCIg4",
        "colab_type": "text"
      },
      "source": [
        "<a id='outlier_removal'></a>\n",
        "#### Outlier removal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dTQwseXCIg4",
        "colab_type": "text"
      },
      "source": [
        "The data often contain outliers. There are many reasons for this:\n",
        "* Data entry errors (human errors)\n",
        "* Measurement errors (device errors) \n",
        "* Experimental errors (data extraction or experiment planning / execution)\n",
        "* Intentional errors (artificial outliers made to test detection methods)\n",
        "* Data processing errors (data manipulation or unintentional mutations in the dataset)\n",
        "* Sampling errors (extracting or mixing of the data from incorrect or different sources)\n",
        "* Noize (not an error, naturally comes from the data)\n",
        "\n",
        "There are different ways to find outliers in the data. They can take into account only one of the features or several of them. Here are some of the methods to find outliers:\n",
        "\n",
        "* Z-Score or Extreme Value Analysis\n",
        "* Probabilistic or statistical modeling\n",
        "* Linear methods (Principal Component Analysis or Least Squares Method)\n",
        "* Models based on data closeness (for example, [Local Outlier Factor](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html))\n",
        "* [Isolation Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html#sklearn.ensemble.IsolationForest)\n",
        "\n",
        "In our workshop we will use the lat one. We will not go into the details of the algorithm. You can read more about it [here](https://medium.com/@hyunsukim_9320/isolation-forest-step-by-step-341b82923168)\n",
        "\n",
        "To identify anomalies by the chosen method, it is necessary to train a special model. This is done in a similar way to ordinary training. We will assume that there are 5% of outliers in our subset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0np5VGqBCIg4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "outlier_det = Pipeline([('scale', StandardScaler()),\n",
        "                        ('clf', IsolationForest(contamination=0.05, \n",
        "                                                n_estimators=100,\n",
        "                                                n_jobs=-1,\n",
        "                                                random_state=123,\n",
        "                                                behaviour='new')\n",
        "                        )])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hZ2ijRECIg4",
        "colab_type": "text"
      },
      "source": [
        "By executing the *it_predict* method we will immediately get an outlier prediction for the training sample. The value -1 means outlier and value 1 - normal sample."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCRYhhs2CIg5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_out = outlier_det.fit_predict(X_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJah6pDtCIg5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.unique(train_out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-22YI3tCIg6",
        "colab_type": "text"
      },
      "source": [
        "Now, we will initialize the mask with no outliers, so we will not take them into account while training. Such a mask is an array that contains True if the example is normal, and False if it is an outlier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJXd2U3BCIg6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "out_mask = train_out == 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAgPgkOVCIg6",
        "colab_type": "text"
      },
      "source": [
        "Let's see if the results become better. When you are training don't forget to use the outliers mask."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEdXYJnUCIg7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(123)\n",
        "\n",
        "with timer:\n",
        "    clf = GridSearchCV(pipe, cv=3, \n",
        "                       param_grid=params, \n",
        "                       scoring='f1_macro', verbose=1,\n",
        "                       n_jobs=6).fit(X_train[out_mask], y_train[out_mask])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awd6JNeYCIg7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_time = timer.elapsed_time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J20Q8i_vCIg8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clf.best_score_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3JBAD4ACIg9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with timer:\n",
        "    predict = clf.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UkDiyeqCIg9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict_time = timer.elapsed_time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5c8fqP_3CIg-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "calculate_metrics(predict, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9lFclwvCIg-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results = results.append({'method': 'Outliers removing',\n",
        "                          'model': 'LR',\n",
        "                          'val score': clf.best_score_,\n",
        "                          'test score': f1_score(clf.predict(X_test), y_test, average='macro'), \n",
        "                          'learning time':learning_time,\n",
        "                          'predict time': predict_time},\n",
        "                          ignore_index=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZb7Z3fnCIhB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BiYJVVLpCIhC",
        "colab_type": "text"
      },
      "source": [
        "The validation accuracy became a little bit lower, but since we have changed the size of the training set, the validation data became different, and it isn't correct to compare these values. But it is clear that the test accuracy has increased. To simplify further manipulations with the code, we will not remove outliers from the data during the workshop. However, you can use an outlier mask in independent tasks and see if the results become better or not."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiC2IC4rCIhC",
        "colab_type": "text"
      },
      "source": [
        "<a id='dimensional_reduction'></a>\n",
        "### Dimensionality reduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qgtIhGxCIhC",
        "colab_type": "text"
      },
      "source": [
        "Usualy it is useful to know what feature are actually doing the work, so we can drop all others as they can actually hurt the final score. In our case there are only 123 features, but it can be tens and hundreds of thousands. The goal is to reduce the dimensionality of the feature space and not lose useful information.\n",
        "\n",
        "There are various ways to reduce the dimension of the data:\n",
        ">*Statistical methods of dimension reduction*\n",
        "1. Principal Component Analysis\n",
        "2. Linear Discriminant Analysis\n",
        "3. Factor analysis\n",
        "\n",
        ">*Machine Learning based algorithms\n",
        "1. T-SNE \n",
        "2. UMAP and others.\n",
        "\n",
        ">*Methods based on feature selection*\n",
        "1. The significance of the features (determined by the training of some models)\n",
        "2. Filtering methods (rank features based on statistic functions - e.g. Chi square test, ANOVA)\n",
        "3. Lasso and Ridge regression\n",
        "4. Greedy algorithm - step by step adding features (Forward Feature Selection) or Backward Feature Elimination\n",
        "\n",
        ">Autoencoders should be noted as a method of reducing the dimension of data. \n",
        "\n",
        "We will look at a couple of these methods: Principal Component Analysis and Backward Feature Elimination."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rciyxJqSCIhD",
        "colab_type": "text"
      },
      "source": [
        "<a id='pca'></a>\n",
        "**Principal Component Analysis** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ue0kdjtCIhD",
        "colab_type": "text"
      },
      "source": [
        "Principal Component Analysis (PCA) - one of the popular ways to reduce data dimensionality with minimum losing of information. The challenge of principal component analysis is to find subspaces of smaller dimension, in orthogonal projection on which the data variation (that is, the standard deviation from the mean value) is maximal.\n",
        "\n",
        "In simple terms, the method finds the n-th number of new axes, projections on which retain as much variance of the original data as possible. The projections themselves become new features.\n",
        " \n",
        "It should be used carefully, because the feature space is radically changing. The information necessary for the algorithms performance can be lost even with a high retention of information content. Also it is very prone to overfitting.\n",
        "\n",
        "Since the method is related to variance of features, it is necessary to scale data before using PCA."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQB66SCNCIhD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = PCA()\n",
        "pca_data = model.fit_transform(StandardScaler().fit_transform(df_new))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BztX1XDfCIhD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=[13, 5])\n",
        "plt.title('Principal Component Analysis (PCA)')\n",
        "plt.plot(range(len(model.explained_variance_ratio_)), model.explained_variance_ratio_, '--o')\n",
        "plt.ylabel('Explained variance ratio')\n",
        "plt.xlabel('Components')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziR6ysE1CIhE",
        "colab_type": "text"
      },
      "source": [
        "Apparently, most of the information can be saved with 60-70 new projections. Let's see how many components do we need to keep 99% of the original dataset's information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SefPmqOkCIhE",
        "colab_type": "text"
      },
      "source": [
        "**Task 18** Apply the PCA method to the *df_new* dataset again, but set the *n_components* parameter to 0.99. If this parameter is an integer, then in the final matrix will be left such a number of features. If it is the float value from 0 to 1, the features will be selected automatically in order to save this amount of information. Print the size of the resulting matrix  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diXMI6X1CIhE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# write your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ULh67oYCIhF",
        "colab_type": "text"
      },
      "source": [
        "<a id='greedy_selection'></a>\n",
        "**Greedy selection**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSa9KdYFCIhF",
        "colab_type": "text"
      },
      "source": [
        "Now we will use the most reliable, but very resource-intensive method - iterative brute force feature selection. We will remove one feature at a time, predict the target variable and see how the quality metric changes: the less the metric decreases after removing the feature, the less important this feature is. More than that, sometimes the metric might even improve.\n",
        "\n",
        "This method takes a lot of time. Therefore, we won't calculate it now, but you can see the code and its results below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yLFHnRNCIhF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %%time\n",
        "\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(df_new, Y,\n",
        "#                                                     test_size=0.3,\n",
        "#                                                     random_state=42)\n",
        "\n",
        "# scaled_data = StandardScaler().fit_transform(X_train)\n",
        "# scaled_X = pd.DataFrame(scaled_data, index=X_train.index, columns=X_train.columns)\n",
        "\n",
        "# clf_logreg = LogisticRegression(random_state=42, C=0.99, penalty='l1')\n",
        "\n",
        "# parameters_grid = {'C': [0.99]}\n",
        "# columns = X_train.columns\n",
        "# important_features = []\n",
        "# features_scores = []\n",
        "# \\\n",
        "\n",
        "# np.random.seed(123)\n",
        "\n",
        "\n",
        "# for j in range(len(columns)-1):\n",
        "#     print('{}\\{}'.format(j, len(columns)))\n",
        "#     col_for_del = []\n",
        "#     scores = []\n",
        "#     for i in columns:\n",
        "#         cols = columns[columns != i]\n",
        "#         clf = GridSearchCV(clf_logreg, cv=3,\n",
        "#                            param_grid=parameters_grid,\n",
        "#                            scoring='f1_macro',\n",
        "#                            verbose=0,\n",
        "#                            n_jobs=6).fit(scaled_X[cols], y_train)\n",
        "\n",
        "#         scores.append(clf.best_score_)\n",
        "#     max_col = columns[np.argmax(scores)]\n",
        "#     print(max_col, clf.best_score_)\n",
        "#     important_features.append(max_col)\n",
        "#     features_scores.append(max(scores))\n",
        "#     columns = columns[columns != max_col]\n",
        "#     print(j, '\\r', end='')\n",
        "# features_scores.append(0)\n",
        "# important_features.append(columns[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xm4HsyJACIhG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# feature_importances = pd.DataFrame({'features': important_features, \n",
        "#                                     'feature_importances': features_scores,\n",
        "#                                     'iteration': range(len(features_scores))})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5LUZJyVCIhG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# feature_importances = feature_importances.sort_values('iteration', ascending=True)\n",
        "# feature_importances.to_csv(FEATURE_IMPORTANCE_F, index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ndol2iQCIhH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature_importances = pd.read_csv(FEATURE_IMPORTANCE_F)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hd44Kg5cCIhI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=[15, 6])\n",
        "plt.title('Brute force')\n",
        "plt.plot(feature_importances.iteration[:-1], \n",
        "         feature_importances.feature_importances[:-1], '-o')\n",
        "plt.ylabel('f1_macro')\n",
        "plt.xlabel('iteration')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LWmEmuKCIhJ",
        "colab_type": "text"
      },
      "source": [
        "Top 20 of the most significant features:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gNRnzizCIhJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature_importances.features[::-1][:20]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVhezBkLCIhK",
        "colab_type": "text"
      },
      "source": [
        "Let's find at what stage was the best accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghaXKasPCIhL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature_importances[feature_importances.feature_importances == feature_importances.feature_importances.max()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1s7ImKkyCIhM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_id = feature_importances[feature_importances.feature_importances == feature_importances.feature_importances.max()].index[0]\n",
        "print('We can leave {} features'.format(len(feature_importances)-max_id))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFMrHRwxCIhN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "selected_features = feature_importances.features[max_id:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Bub4sWnCIhO",
        "colab_type": "text"
      },
      "source": [
        "[Table of contents](#0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1E5p8uXyCIhR",
        "colab_type": "text"
      },
      "source": [
        "<a id='4_final_prediction'></a>\n",
        "## 4.Final prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bePBNreuCIhS",
        "colab_type": "text"
      },
      "source": [
        "**Task 19** Train the model on the features obtained by the greedy selection method and the PCA method. Add the results to the table."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvrXkPAACIhS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Greedy features\n",
        "# write your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xjjZqPqCIhU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results = results.append({'method': 'Greedy selection',\n",
        "                          'model': 'LR',\n",
        "                          'val score': clf.best_score_,\n",
        "                          'test score': f1_score(clf.predict(X_test), y_test, average='macro'), \n",
        "                          'learning time': learning_time,\n",
        "                          'predict time': predict_time},\n",
        "                          ignore_index=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wpgo63UXCIhX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# PCA features\n",
        "# write your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lYQ-Q1DCIhY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results = results.append({'method': 'PCA features',\n",
        "                          'model': 'LR',\n",
        "                          'val score': clf.best_score_,\n",
        "                          'test score': f1_score(clf.predict(X_test), y_test, average='macro'), \n",
        "                          'learning time': learning_time,\n",
        "                          'predict time': predict_time},\n",
        "                          ignore_index=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLwd7_i5CIhZ",
        "colab_type": "text"
      },
      "source": [
        "<a id='conclusions'></a>\n",
        "### Conclusion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cHf0hLNCIhZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3D9Cm9CCIhZ",
        "colab_type": "text"
      },
      "source": [
        "The best result on the validation set was shown by a model trained on the features obtained by the greedy selection method. Because some features can decrease the accuracy, removing them allows you to get a better result than on the original dataset. However, on the test set, the greedy algorithm showed slightly worse results than the full dataset or dataset without outliers. This could happen due to the fact that the optimal set of features was selected for the validation set which in our case might have a slightly different distribution and the dataset isn't particularly large. \n",
        "\n",
        "The model trained on the features obtained with the PCA method provides accuracy comparable to results provided by the models trained on raw data, but at the same time it learns much faster and the difference between val and test scores is negligible.\n",
        "\n",
        "We can't say that some approach is definitely better than others, it depends on the task. In some cases, greedy selection can significantly improve the result, but the time it takes can be extremely long and it can overfit. Using the principal component analysis allows you quickly and easily reduce the dimensionality, which can drastically speed up the training and prediction processes, however there may be a loss in accuracy. There are a lot of other algorithms to perform these actions, from this point it's for you to decide which result is more suitable for you.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDcBmm17CIhZ",
        "colab_type": "text"
      },
      "source": [
        "<a id='bonus'></a>\n",
        "### Bonus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "om9hwXJcCIha",
        "colab_type": "text"
      },
      "source": [
        "Below you can find an example of comparative training of different models on different sets of features with a wide random selection of hyperparameters. Training takes a lot of time, so we have commented the code. You can load and view the results at the very end of the script. These results were obtained on a slightly different set of features, so they may differ from the results obtained in the workshop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KPLr06yCIha",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "# from catboost import CatBoostClassifier\n",
        "# from lightgbm import LGBMClassifier\n",
        "\n",
        "# log_params = {'clf__C': uniform(0.01, 0.99),\n",
        "#               'clf__penalty': ['l1', 'l2']} \n",
        "\n",
        "# rf_params = {'clf__n_estimators': randint(100, 1000),\n",
        "#              'clf__max_depth': randint(1,10),\n",
        "#              'clf__min_samples_leaf': randint(1, 10),\n",
        "#              'clf__min_samples_split': randint(2, 11)} \n",
        "\n",
        "# gb_params = {'clf__learning_rate': uniform(0.01, 0.49),\n",
        "#              'clf__n_estimators': randint(100, 700),\n",
        "#              'clf__max_depth': randint(1, 10),\n",
        "#              'clf__subsample': uniform(0.6, 0.4)}   \n",
        "\n",
        "# models_dict = {'Logistic Regression':[log_params, LogisticRegression(random_state=42)],\n",
        "#                'Random Forest':[rf_params, RandomForestClassifier(random_state=42)],\n",
        "#                'Gradient Boosting': [gb_params, GradientBoostingClassifier(random_state=42)],\n",
        "#                'XGB':[gb_params, xgb.XGBClassifier(random_state=42)],\n",
        "#                'CatBoost':[gb_params, CatBoostClassifier(bootstrap_type='Bernoulli', random_state=42, verbose=0)],\n",
        "#                'LightBoost':[gb_params, LGBMClassifier(random_state=42)]}\n",
        "\n",
        "# ext_results = pd.DataFrame(columns=['method', 'model', 'val score', 'test score', 'learning time', 'predict time'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSVpjYCgCIhb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def custom_pipe(clf, pca='False'):\n",
        "#     if pca:\n",
        "#         return  Pipeline([('clf', clf)])\n",
        "#     else:\n",
        "#         return Pipeline([('scale', StandardScaler()),\n",
        "#                          ('clf', clf)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-lbULlQCIhd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# X_train_gs, X_test_gs = train_test_split(df_new[selected_features],test_size=0.3, random_state=42)\n",
        "# X_train_all, X_test_all = train_test_split(df_new,test_size=0.3, random_state=42)\n",
        "\n",
        "# pca_model = PCA(n_components=0.99)\n",
        "# scaler = StandardScaler()\n",
        "# X_train_pca = pca_model.fit_transform(scaler.fit_transform(X_train_all))\n",
        "# X_test_pca = pca_model.transform(scaler.transform(X_test_all))\n",
        "\n",
        "# data_dict = {'Manual features':(X_train_all, X_test_all),  \n",
        "#              'Greedy selection':(X_train_gs, X_test_gs),\n",
        "#              'PCA':(X_train_pca, X_test_pca)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k61MEe0lCIhe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# np.random.seed(123)\n",
        "# for method, (X_train, X_test) in data_dict.items():\n",
        "#     print(f'Method: {method}\\n')\n",
        "    \n",
        "#     if method == \"PCA\":\n",
        "#         pca_flag = True\n",
        "#     else:\n",
        "#         pca_flag = False\n",
        "    \n",
        "#     for clf_name, vals in models_dict.items():\n",
        "#         print(clf_name, '\\n')\n",
        "\n",
        "#         pipe = custom_pipe(vals[1], pca_flag)\n",
        "        \n",
        "#         with timer:\n",
        "#             clf = RandomizedSearchCV(pipe, cv=3, random_state=123, \n",
        "#                                      param_distributions=vals[0], \n",
        "#                                      n_jobs=4,\n",
        "#                                      verbose=1,\n",
        "#                                      n_iter=100,\n",
        "#                                      scoring='f1_macro').fit(X_train, y_train)\n",
        "\n",
        "#         print(clf.best_params_ , '\\n')\n",
        "\n",
        "#         learning_time = int(timer.elapsed_time / 300)\n",
        "\n",
        "#         with timer:\n",
        "#             predict = clf.predict(X_test)\n",
        "\n",
        "#         predict_time = timer.elapsed_time\n",
        "\n",
        "#         ext_results = ext_results.append({'method': method,\n",
        "#                                           'model': clf_name,\n",
        "#                                           'val score': clf.best_score_,\n",
        "#                                           'test score': f1_score(predict, y_test, average='macro'), \n",
        "#                                           'learning time': learning_time,\n",
        "#                                           'predict time': predict_time},\n",
        "#                                           ignore_index=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhGgB98FCIhe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ext_results.to_csv(BONUS_F, index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypYoKKY0CIhf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ext_results = pd.read_csv(BONUS_F)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32lM0IWYCIhg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ext_results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FRuGkpZCIhh",
        "colab_type": "text"
      },
      "source": [
        "Training was carried out with a wide choice of parameters. The best result on the validation set was shown by CatBoost on all the features. It also showed the best result on the test set, but on the features obtained with greedy selection. In general, models based on gradient boosting show a similar results. Random Forest is a little behind them. Logistic Regression gives the weakest results.\n",
        "\n",
        "\n",
        "The accuracy of the models trained on the features obtained with the PCA is lower than in other cases. This is especially evident in the test sample. At the same time, training on them wasn't faster than on other methods, and sometimes even more. \n",
        "\n",
        "The comparison of learning time of gradient boosting models with RandomizedSearchCV is not absolutely accurate, as it depends on the *n_estimators* hyperparameter and it is different for each case. \n",
        "\n",
        "But in general, it can be noted that CatBoost is trained much longer than other gradient methods. However, it predicts very fast. LightBoost is the fastest learning gradient boosting model. In general, the fastest model in learning and prediction is a logistic regression. So if time is more valuable than accuracy, linear methods are the best choice. If you still want a sufficient accuracy, you should use LightBoost"
      ]
    }
  ]
}